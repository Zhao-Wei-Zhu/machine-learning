{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Download torchvision, huggingface's evaluate and transformers. Make sure that accelerate's version is compatible between transformers and torchvision.\n",
        "We also load scikit image to load the images."
      ],
      "metadata": {
        "id": "xjAyKVh0vCB1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBHZwI5cu67m"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet scikit-image\n",
        "!pip install --quiet matplotlib\n",
        "!pip install --quiet torchvision\n",
        "!pip install --quiet evaluate\n",
        "!pip install --quiet transformers\n",
        "!pip install --quiet accelerate>=0.21.0 -U"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Colab's session by downloading the data from drive\n",
        "Since we are using colab, the data is deleted every session connection. We download the data from google drive every time to prepare the notebook for running."
      ],
      "metadata": {
        "id": "T1IMHTq8vJ8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown, os\n",
        "\n",
        "DATASET_ROOT_DIR = \"/content/data\"\n",
        "\n",
        "if not os.path.exists(DATASET_ROOT_DIR):\n",
        "\n",
        "  file_id = \"1Z5ZjEdrOvgcHdU6FSpzG53oQ0QlCGv3-\"\n",
        "  gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output=f\"/content/\")\n",
        "  !unzip /content/data.zip"
      ],
      "metadata": {
        "id": "KRBU9HGivGMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from itertools import chain\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from skimage import io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import numpy as np # for transformation\n",
        "import torch # PyTorch package\n",
        "import torchvision.transforms as transforms # transform data\n",
        "import torch.nn as nn # basic building block for neural neteorks\n",
        "import torch.nn.functional as F # import convolution functions like Relu\n",
        "import torch.optim as optim # optimzer\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "\n",
        "from transformers import TrainingArguments, Trainer, DefaultDataCollator\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold"
      ],
      "metadata": {
        "id": "hr7kVNV5vUwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Modeling Configurations\n",
        "\n",
        "BATCH_SIZE = 8 # If you have less ram, use 4"
      ],
      "metadata": {
        "id": "3Bdz_OPCvsHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Definition\n",
        "\n",
        "We use a custom dataset that loads the images on demand in order to save up RAM. Data is represented as a lookup table saving only the image path and label in memory, and loading image by image on demand.\n",
        "\n",
        "First, we prepare a table in Panda's dataframe format that contains the image's information:\n",
        "1. Root directory: the folder containing the image (e.g. /data/cv)\n",
        "2. Image file name: the name of the image file. All images are in `.tif` extension\n",
        "3. text_label: the label of the image. It is one of: ` 'ad', 'cv', 'doc', 'email', 'other'`"
      ],
      "metadata": {
        "id": "8X6PWmnjwghF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(root_dir:str):\n",
        "\n",
        "  all_paths = os.walk(root_dir)\n",
        "  all_paths = list(all_paths)\n",
        "\n",
        "  img_paths_tabularized = [ [{ \"parent_dir\": path_[0], 'img_path': img_ } for img_ in path_[-1]]  for path_ in all_paths[1:] ]\n",
        "  img_paths_tabularized  = list(chain.from_iterable(img_paths_tabularized))\n",
        "  print(f\"[Dataset][INFO] Preparing paths for {len(img_paths_tabularized)} images\")\n",
        "\n",
        "  images_lookup = pd.DataFrame.from_dict(img_paths_tabularized)\n",
        "  images_lookup['text_label'] = images_lookup['parent_dir'].apply(lambda x: x.strip().split(\"/\")[-1])\n",
        "\n",
        "  label2id = { doc_type:id_   for id_, doc_type in enumerate([ 'ad', 'cv', 'doc', 'email', 'other' ]) }\n",
        "  id2label = { id_:doc_type   for doc_type, id_ in label2id.items() }\n",
        "\n",
        "  return images_lookup, label2id, id2label"
      ],
      "metadata": {
        "id": "6pITq4bnwhYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second, we define out custom dataset class `DocumentTypeDataset`.\\\n",
        "This class takes as input a dataframe containing the image information and a label-to-id mapping dictionary which is used to provide numerical number for each class.\n",
        "It also accepts an image transformation pipeline.\n",
        "\n",
        "In order to save RAM memory, this dataset loads the images on demand. We only keep the path to the image in-memory, and provide the images only when they are required."
      ],
      "metadata": {
        "id": "ilL2JfyTxIWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentTypeDataset(Dataset):\n",
        "  \"\"\"Document Type Dataset.\"\"\"\n",
        "\n",
        "  def __init__(self,data_lookup:pd.DataFrame, label2id:dict, img_transformation=None):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        data_lookup: the dataframe of image paths and corresponding labels\n",
        "        label2id: dictionary to map the labels to their corresponding Ids\n",
        "        transform (callable, optional): Optional transformation to be applied\n",
        "            on a sample, if any.\n",
        "    \"\"\"\n",
        "    self.img_transformation = img_transformation\n",
        "\n",
        "    self.images_lookup = data_lookup\n",
        "    self.label2id = label2id\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images_lookup)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\" Load images from their corresponding path.  \"\"\"\n",
        "\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "\n",
        "    # construct the image path from the parent dir (e.g. `/data/cv`) and the image name (e.g.  `image_1.tif`)\n",
        "    img_name = os.path.join(self.images_lookup.iloc[idx]['parent_dir'], self.images_lookup.iloc[idx]['img_path'])\n",
        "\n",
        "    # read the image from disk using scikit-image. It reads the image in greyscale by default.\n",
        "    image = io.imread(img_name)\n",
        "    # construct the data record <X, Y> where X is the image and Y is the corresponding true label\n",
        "    sample = {'image': image, 'label': self.label2id[  self.images_lookup.iloc[idx]['text_label'] ] }\n",
        "    # If we need to apply any transformation to the image, apply them\n",
        "    if self.img_transformation:\n",
        "      sample['image'] = self.img_transformation(sample['image'])\n",
        "\n",
        "    return sample"
      ],
      "metadata": {
        "id": "tIZoP-q3xPmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to apply some transformations to the images to enhance the data quality and standardize them. The transformations we use are:\n",
        "1. **ToTensor():** make sure that the image is in pytorch tensor format\n",
        "2. **Resize():** Images of different sizes can confuse the model and reduce its performane. To avoid different image sizes, we resize all images to a fixed size of 64*64.\n",
        "3. Normalize(): To facilitate learning the distribution of every class in the dataset, we normalize the images into μ=0.5, σ=0.5"
      ],
      "metadata": {
        "id": "cENIFJ3VEXxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The image transformation pipleine\n",
        "transformation_pipeline = transforms.Compose( #* composing several transforms together\n",
        "    [transforms.ToTensor(), #* to tensor object\n",
        "     transforms.Resize((64,64)),\n",
        "     transforms.Normalize(0.5, 0.5)]) #* mean = 0.5, std = 0.5\n"
      ],
      "metadata": {
        "id": "s7mMR1_AEaEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the image lookup to the entire dataset. This will be split into train-validation in the K-fold cross validation step later on."
      ],
      "metadata": {
        "id": "hXmKFDwnEcF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images_lookup, label2id, id2label = prepare_data(DATASET_ROOT_DIR)"
      ],
      "metadata": {
        "id": "f_b-TIT2EeFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data collator used by HuggingFace's model trainer to stack data."
      ],
      "metadata": {
        "id": "I5PO1og0EgLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DefaultDataCollator(return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "V8wpPcMuEiKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building"
      ],
      "metadata": {
        "id": "Jp15FVY0EkQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use a Convolutional Neural Network as to build our model and train it from scratsh. Since we have relatively moderate amount of data, we use a medium-sized CNN architecture. The model consists of:\n",
        "1. The convolution bolcks consisting of:\n",
        "  1. First 2D Convolution layer consisting of 3*3 kernels and 16 output channels. We use this small sized kernel to help the model learn local image information at early stage.\n",
        "  2. 3*3 2D-MaxPooling layer.\n",
        "  3. Second 2D convolution layer consisting of 5*5 kernels and 5 output channels. We use a larger kernel than the first one to allow the model understand more general features of the images.\n",
        "  4. 2*2 2D-MaxPooling Layer.\n",
        "\n",
        "2. classification block consisting of:\n",
        "  1. First Fully Connected Layer of size 128.\n",
        "  2. Second Fully Connected Layer of size 64.   \n",
        "  3. Output layer consisting of 5 neurons for 5 classes.\n",
        "\n",
        "For all layers we use LeakyReLU Activation."
      ],
      "metadata": {
        "id": "vylfOqqMEmv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "  ''' Simple Convolutional Neural Network'''\n",
        "\n",
        "  def __init__(self):\n",
        "    ''' initialize the network '''\n",
        "    super(CNNModel, self).__init__()\n",
        "\n",
        "    self.convolution_blocks = nn.Sequential(\n",
        "        nn.Conv2d(1, 16, 3), # we have a single input channel, and a 6 output channel. Use 5*5 convolution kernel\n",
        "        nn.LeakyReLU(),\n",
        "        nn.MaxPool2d(3,3),\n",
        "        nn.Conv2d(16, 5, 5),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.MaxPool2d(2,2)\n",
        "    )\n",
        "\n",
        "    self.flattening_length = 5*8*8\n",
        "\n",
        "    self.classification_head = nn.Sequential(\n",
        "        nn.Linear(self.flattening_length, 128),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.Linear(128,64),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.Linear(64, 5) # we have 5 output classes\n",
        "    )\n",
        "\n",
        "    self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "  def forward(self, image, labels):\n",
        "    ''' the forward propagation algorithm '''\n",
        "    x = self.convolution_blocks(image)\n",
        "    #print(x.shape)\n",
        "    x = x.view(-1, self.flattening_length)\n",
        "    x = self.classification_head(x)\n",
        "\n",
        "    one_hot_labels = F.one_hot(labels, num_classes=5)\n",
        "    loss = self.loss_fn( x, one_hot_labels.float() )\n",
        "    return { 'output':x, 'loss': loss}\n",
        "\n",
        "net = CNNModel()\n",
        "print(net)"
      ],
      "metadata": {
        "id": "hKk2X4kIEoEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Utilities"
      ],
      "metadata": {
        "id": "OoxkZ3FEErvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every training epoch, we need to evaluate the model's performance on the validation set. This helps us see if the model is improving and whether it starts to overfit on the training data or not.\n",
        "\n",
        "Since we are using a classification task, the best metrics are the Accuracy and F1-score. For the F1-score, we report the macro-average."
      ],
      "metadata": {
        "id": "UKgxO3ueEue3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Evaluation Methods:\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "f1_score = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return { 'accuracy': accuracy.compute(predictions=predictions, references=labels)['accuracy'],\n",
        "            'f1': f1_score.compute(predictions=predictions, references=labels, average='macro')['f1']\n",
        "    }"
      ],
      "metadata": {
        "id": "U3e1oZX-Ew3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop with K-folds Cross Validation"
      ],
      "metadata": {
        "id": "ULRIOT9PE1cz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we use K-fold Cross Validation to assess the model's performance, with k=5. Since we have imbalanced classes, we use the Stratified version of the K-fold CV. This way each fold has the same distribution of classes as the overall dataset, hence giving more stable and meaningful results."
      ],
      "metadata": {
        "id": "XOk4877CE4Ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=5331) # initialize the K-fold CV object.\n",
        "\n",
        "X = images_lookup[['parent_dir', 'img_path']]\n",
        "Y = images_lookup['text_label']\n",
        "\n",
        "\n",
        "\n",
        "for i, (train_index, test_index) in enumerate(skf.split(X, Y)):\n",
        "  ## Every fold, construct the training and validation data subsets.\n",
        "\n",
        "  print(f'{\"-\"*7} Fold #{i+1} { \"-\"*7 }')\n",
        "  training_lookup, testing_lookup = images_lookup.iloc[train_index], images_lookup.iloc[test_index]\n",
        "  training_data = DocumentTypeDataset(training_lookup, label2id, transformation_pipeline)\n",
        "  validation_data = DocumentTypeDataset(testing_lookup, label2id, transformation_pipeline)\n",
        "  # reset model every fold\n",
        "  net = CNNModel()\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "    output_dir=f\"/content/cnn_model/fold#{i}\",\n",
        "    remove_unused_columns=False,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=1e-3, # Use a relatively small learning rate to avoid overfitting on the data.\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=1,\n",
        "    num_train_epochs=10, # this is the best number of epochs to train the model without allowing it to overfit.\n",
        "    warmup_ratio=0.1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\", # The best model is the highest F1 score not the lowest.\n",
        "    push_to_hub=False,\n",
        "  )\n",
        "\n",
        "  trainer = Trainer(\n",
        "    model=net,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=training_data,\n",
        "    eval_dataset=validation_data,\n",
        "    compute_metrics=compute_metrics,\n",
        "  )\n",
        "  trainer.train()"
      ],
      "metadata": {
        "id": "V65Lcy95E7vS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}